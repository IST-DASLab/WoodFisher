pruners:
  pruner_1:
    class: BlockwiseWoodburryFisherPruner
    epochs: [1,5,40]
    weight_only: True
    initial_sparsity: 0.05
    target_sparsity: 0.90
    modules: [
      layer1.0.conv1,
      layer1.0.conv2,
      layer1.0.conv3,
      layer1.0.downsample.0,
      layer1.1.conv1,
      layer1.1.conv2,
      layer1.1.conv3,
      layer1.2.conv1,
      layer1.2.conv2,
      layer1.2.conv3,
      layer2.0.conv1,
      layer2.0.conv2,
      layer2.0.conv3,
      layer2.0.downsample.0,
      layer2.1.conv1,
      layer2.1.conv2,
      layer2.1.conv3,
      layer2.2.conv1,
      layer2.2.conv2,
      layer2.2.conv3,
      layer2.3.conv1,
      layer2.3.conv2,
      layer2.3.conv3,
      layer3.0.conv1,
      layer3.0.conv2,
      layer3.0.conv3,
      layer3.0.downsample.0,
      layer3.1.conv1,
      layer3.1.conv2,
      layer3.1.conv3,
      layer3.2.conv1,
      layer3.2.conv2,
      layer3.2.conv3,
      layer3.3.conv1,
      layer3.3.conv2,
      layer3.3.conv3,
      layer3.4.conv1,
      layer3.4.conv2,
      layer3.4.conv3,
      layer3.5.conv1,
      layer3.5.conv2,
      layer3.5.conv3,
      layer4.0.conv1,
      layer4.0.conv2,
      layer4.0.conv3,
      layer4.0.downsample.0,
      layer4.1.conv1,
      layer4.1.conv2,
      layer4.1.conv3,
      layer4.2.conv1,
      layer4.2.conv2,
      layer4.2.conv3,
    ]
    keep_pruned: False
  pruner_2:
    class: BlockwiseWoodburryFisherPruner
    epochs: [1,5,40]
    weight_only: True
    initial_sparsity: 0.05
    target_sparsity: 0.80
    modules: [fc]
    keep_pruned: False

trainers:
  default_trainer:
    optimizer:
      class: SGD
      lr: 0.005
      momentum: 0.9
      weight_decay: 0.0001

    lr_scheduler:
      class: ExponentialLR
      gamma: 0.6
      epochs: [40,6,90]